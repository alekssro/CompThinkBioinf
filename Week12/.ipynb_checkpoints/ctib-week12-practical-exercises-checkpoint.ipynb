{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math  # Just ignore this :-)\n",
    "\n",
    "def log(x):\n",
    "    if x == 0:\n",
    "        return float('-inf')\n",
    "    return math.log(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CTiB - Week 12 - Practical Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the exercise below, you will implement and experiment with training-by-counting as a way to select the parameters of an HMM (i.e. training) as explained in the lectures in week 12."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 1 - Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below you will implement and experiment with estimating the parameters (transition, start, and emission probabilities) for an HMM from data (sequences of observations, ${\\bf X}$, and corresponding sequences of hidden states, ${\\bf Z}$, using the training-by-counting method. \n",
    "\n",
    "We will be working with the 7-state model (`hmm_7_state`) model that we also worked with last time. The model is included below. \n",
    "\n",
    "We will use your implementation of the Viterbi algorithm (`compute_w` and `opt_path_prob`) from week 10 to investiage your trained models, so you need to add them below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class hmm:\n",
    "    def __init__(self, init_probs, trans_probs, emission_probs):\n",
    "        self.init_probs = init_probs\n",
    "        self.trans_probs = trans_probs\n",
    "        self.emission_probs = emission_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_probs_7_state = [0.00, 0.00, 0.00, 1.00, 0.00, 0.00, 0.00]\n",
    "\n",
    "trans_probs_7_state = [\n",
    "    [0.00, 0.00, 0.90, 0.10, 0.00, 0.00, 0.00],\n",
    "    [1.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00],\n",
    "    [0.00, 1.00, 0.00, 0.00, 0.00, 0.00, 0.00],\n",
    "    [0.00, 0.00, 0.05, 0.90, 0.05, 0.00, 0.00],\n",
    "    [0.00, 0.00, 0.00, 0.00, 0.00, 1.00, 0.00],\n",
    "    [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00],\n",
    "    [0.00, 0.00, 0.00, 0.10, 0.90, 0.00, 0.00],\n",
    "]\n",
    "\n",
    "emission_probs_7_state = [\n",
    "    #   A     C     G     T\n",
    "    [0.30, 0.25, 0.25, 0.20],\n",
    "    [0.20, 0.35, 0.15, 0.30],\n",
    "    [0.40, 0.15, 0.20, 0.25],\n",
    "    [0.25, 0.25, 0.25, 0.25],\n",
    "    [0.20, 0.40, 0.30, 0.10],\n",
    "    [0.30, 0.20, 0.30, 0.20],\n",
    "    [0.15, 0.30, 0.20, 0.35],\n",
    "]\n",
    "\n",
    "hmm_7_state = hmm(init_probs_7_state, trans_probs_7_state, emission_probs_7_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need the helper functions for translating between observations/paths and indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_path_to_indices(path):\n",
    "    return list(map(lambda x: int(x), path))\n",
    "\n",
    "def translate_indices_to_path(indices):\n",
    "    return ''.join([str(i) for i in indices])\n",
    "\n",
    "def translate_observations_to_indices(obs):\n",
    "    mapping = {'a': 0, 'c': 1, 'g': 2, 't': 3}\n",
    "    return [mapping[symbol.lower()] for symbol in obs]\n",
    "\n",
    "def translate_indices_to_observations(indices):\n",
    "    mapping = ['a', 'c', 'g', 't']\n",
    "    return ''.join(mapping[idx] for idx in indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, you're given the function below that constructs a table of a specific size filled with zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_table(m, n):\n",
    "    \"\"\"Make a table with `m` rows and `n` columns filled with zeros.\"\"\"\n",
    "    return [[1] * n for _ in range(m)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll be testing your code with the same two sequences as last time, i.e:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_short = 'GTTTCCCAGTGTATATCGAGGGATACTACGTGCATAGTAACATCGGCCAA'\n",
    "z_short = '33333333333321021021021021021021021021021021021021'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_long = 'TGAGTATCACTTAGGTCTATGTCTAGTCGTCTTTCGTAATGTTTGGTCTTGTCACCAGTTATCCTATGGCGCTCCGAGTCTGGTTCTCGAAATAAGCATCCCCGCCCAAGTCATGCACCCGTTTGTGTTCTTCGCCGACTTGAGCGACTTAATGAGGATGCCACTCGTCACCATCTTGAACATGCCACCAACGAGGTTGCCGCCGTCCATTATAACTACAACCTAGACAATTTTCGCTTTAGGTCCATTCACTAGGCCGAAATCCGCTGGAGTAAGCACAAAGCTCGTATAGGCAAAACCGACTCCATGAGTCTGCCTCCCGACCATTCCCATCAAAATACGCTATCAATACTAAAAAAATGACGGTTCAGCCTCACCCGGATGCTCGAGACAGCACACGGACATGATAGCGAACGTGACCAGTGTAGTGGCCCAGGGGAACCGCCGCGCCATTTTGTTCATGGCCCCGCTGCCGAATATTTCGATCCCAGCTAGAGTAATGACCTGTAGCTTAAACCCACTTTTGGCCCAAACTAGAGCAACAATCGGAATGGCTGAAGTGAATGCCGGCATGCCCTCAGCTCTAAGCGCCTCGATCGCAGTAATGACCGTCTTAACATTAGCTCTCAACGCTATGCAGTGGCTTTGGTGTCGCTTACTACCAGTTCCGAACGTCTCGGGGGTCTTGATGCAGCGCACCACGATGCCAAGCCACGCTGAATCGGGCAGCCAGCAGGATCGTTACAGTCGAGCCCACGGCAATGCGAGCCGTCACGTTGCCGAATATGCACTGCGGGACTACGGACGCAGGGCCGCCAACCATCTGGTTGACGATAGCCAAACACGGTCCAGAGGTGCCCCATCTCGGTTATTTGGATCGTAATTTTTGTGAAGAACACTGCAAACGCAAGTGGCTTTCCAGACTTTACGACTATGTGCCATCATTTAAGGCTACGACCCGGCTTTTAAGACCCCCACCACTAAATAGAGGTACATCTGA'\n",
    "z_long = '3333321021021021021021021021021021021021021021021021021021021021021021033333333334564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564563210210210210210210210210210210210210210210210210210210210210210210210210210210210210210210210210210210210210210210210210210210210321021021021021021021021033334564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564563333333456456456456456456456456456456456456456456456456456456456456456456456456456456456456456456456456456456456456456456332102102102102102102102102102102102102102102102102102102102102102102102102102102102102102102102103210210210210210210210210210210210210210210210210210210210210210'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember to translate these sequences to indices before using them with your algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your implementations of compute_w_log and opt_path_prob_log from week 10\n",
    "\n",
    "def compute_w_log(model, x):\n",
    "    k = len(model.init_probs)\n",
    "    n = len(x)\n",
    "    \n",
    "    w = make_table(k, n)\n",
    "    \n",
    "    # Base case: fill out w[i][0] for i = 0..k-1\n",
    "    for i in range(k):\n",
    "        w[i][0] = log(model.init_probs[i])+log(model.emission_probs[i][x[0]])\n",
    "#        print(w)\n",
    "\n",
    "\n",
    "    # Inductive case: fill out w[i][j] for i = 0..k, j = 0..n-1\n",
    "    # ...\n",
    "\n",
    "    for j in range(1,n):\n",
    "        for i in range(k):\n",
    "            for h in range(k):\n",
    "                total = 0\n",
    "                total += w[h][j-1]\n",
    "                total += log(model.trans_probs[h][i])\n",
    "                total += log(model.emission_probs[i][x[j]])\n",
    "                w[i][j] = max(w[i][j], total)\n",
    "  #              print(w)\n",
    "                \n",
    "    return w    \n",
    "\n",
    "\n",
    "\n",
    "def opt_path_prob_log(w):\n",
    "    big = float('-inf')\n",
    "    for i in range(len(w)):\n",
    "            big = max(big, w[i][len(w[i])-1])\n",
    "    return big"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Training-by-counting using `x_short` and `z_short`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume that (`x_short`, `z_short`) is our given training data. Estimate the parameters of a 7-state hmm (`hmm_7_state_tbc_short`) as declared below from this data using training-by-counting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declaration of an 'empty' 7-state HMM, i.e. one where all params are set to zero\n",
    "\n",
    "\n",
    "#Prob for starting in one of the states.\n",
    "init_probs_7_state_tbc_short = [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00]\n",
    "\n",
    "\n",
    "# one row = going from that z state to all the other z states possible\n",
    "trans_probs_7_state_tbc_short = [\n",
    "    [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00],\n",
    "    [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00],\n",
    "    [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00],\n",
    "    [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00],\n",
    "    [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00],\n",
    "    [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00],\n",
    "    [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00],\n",
    "]\n",
    "\n",
    "\n",
    "#\n",
    "emission_probs_7_state_tbc_short = [\n",
    "    #   A     C     G     T\n",
    "    [0.00, 0.00, 0.00, 0.00],\n",
    "    [0.00, 0.00, 0.00, 0.00],\n",
    "    [0.00, 0.00, 0.00, 0.00],\n",
    "    [0.00, 0.00, 0.00, 0.00],\n",
    "    [0.00, 0.00, 0.00, 0.00],\n",
    "    [0.00, 0.00, 0.00, 0.00],\n",
    "    [0.00, 0.00, 0.00, 0.00],\n",
    "]\n",
    "\n",
    "hmm_7_state_tbc_short = hmm(init_probs_7_state_tbc_short, trans_probs_7_state_tbc_short, emission_probs_7_state_tbc_short)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.05263157894736842, 0.05263157894736842, 0.6842105263157895, 0.05263157894736842, 0.05263157894736842, 0.05263157894736842, 0.05263157894736842], [0.6842105263157895, 0.05263157894736842, 0.05263157894736842, 0.05263157894736842, 0.05263157894736842, 0.05263157894736842, 0.05263157894736842], [0.05, 0.7, 0.05, 0.05, 0.05, 0.05, 0.05], [0.05263157894736842, 0.05263157894736842, 0.10526315789473684, 0.631578947368421, 0.05263157894736842, 0.05263157894736842, 0.05263157894736842], [0.14285714285714285, 0.14285714285714285, 0.14285714285714285, 0.14285714285714285, 0.14285714285714285, 0.14285714285714285, 0.14285714285714285], [0.14285714285714285, 0.14285714285714285, 0.14285714285714285, 0.14285714285714285, 0.14285714285714285, 0.14285714285714285, 0.14285714285714285], [0.14285714285714285, 0.14285714285714285, 0.14285714285714285, 0.14285714285714285, 0.14285714285714285, 0.14285714285714285, 0.14285714285714285]]\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "x_short = 'GTTTCCCAGTGTATATCGAGGGATACTACGTGCATAGTAACATCGGCCAA'\n",
    "z_short = '33333333333321021021021021021021021021021021021021'\n",
    "\n",
    "\n",
    "\n",
    "def init_probs_fill(z, K):\n",
    "    res = [0 for i in range(K)]\n",
    "    res[z[0]] = 1\n",
    "    return res\n",
    "\n",
    "m = init_probs_fill(translate_path_to_indices(z_short), 7)\n",
    "#print(m)\n",
    "\n",
    "\n",
    "def trans_probs_fill(z_int, K):\n",
    "    #fill out matrix with counts\n",
    "    matrix_trans = make_table(K,K)\n",
    "    for i in range(len(z_int)-1):\n",
    "            curr_state = z_int[i]\n",
    "            next_state = z_int[i+1]\n",
    "            matrix_trans[curr_state][next_state] += 1\n",
    "    \n",
    "    #Make list of sums of rows in matrix\n",
    "    lst_sum = []\n",
    "    for lst in matrix_trans:\n",
    "        lst_sum.append(sum(lst))\n",
    "        \n",
    "    #Divide all values in list in matrix with the corresponding index in the list of sums.   \n",
    "    for i in range(K):\n",
    "        for j in range(K):\n",
    "            matrix_trans[i][j] = matrix_trans[i][j] / lst_sum[i]\n",
    "    \n",
    "    return print(matrix_trans)\n",
    "\n",
    "print(trans_probs_fill(translate_path_to_indices(z_short), 7))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now adjust the parameters of the above model to reflect the training data (`x_short`, `z_short`). You can probably do this by hand since the sequences are quite short. You can of course also implement some code for doing it.\n",
    "\n",
    "**Explain to another student how you do this.**\n",
    "\n",
    "Adjust the parameters in the above declaration of `hmm_7_state_tbc_short` to the parameters that you get by training-by-counting. (Remember to validate that they are legal parameters.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now Compute the probability of the Viterbi decoding of `x_short` using your trained model and the 'original' 7-state model from the previous weeks, i.e.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-11-757830eb71e0>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-11-757830eb71e0>\"\u001b[1;36m, line \u001b[1;32m3\u001b[0m\n\u001b[1;33m    print opt_path_prob_log(w)\u001b[0m\n\u001b[1;37m                          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Probability of the Viterbi decoding of x_short using hmm_7_state_tbc_short\n",
    "w = compute_w_log(hmm_7_state_tbc_short, translate_observations_to_indices(x_short))\n",
    "print opt_path_prob_log(w)\n",
    "\n",
    "# Probability of the Viterbi decoding of x_short using hmm_7_state\n",
    "w = compute_w_log(hmm_7_state, translate_observations_to_indices(x_short))\n",
    "print opt_path_prob_log(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do the two probabilities compare? What do you expect?\n",
    "\n",
    "Now compute the probability of the Viterbi decoding of `x_long` using the same two models, i.e.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Probability of the Viterbi decoding of x_long using hmm_7_state_tbc_short\n",
    "w = compute_w_log(hmm_7_state_tbc_short, translate_observations_to_indices(x_long))\n",
    "print opt_path_prob_log(w)\n",
    "\n",
    "# Probability of the Viterbi decoding of x_long using hmm_7_state\n",
    "w = compute_w_log(hmm_7_state, translate_observations_to_indices(x_long))\n",
    "print opt_path_prob_log(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do the two probabilities compare? What do you expect?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Training-by-counting using `x_long` and `z_long`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we want to redo what we did above, but with (`x_long`, `z_long`) is our given training data. Estimate the parameters of a 7-state hmm (`hmm_7_state_tbc_long`) as declared below from this data using training-by-counting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Declaration of an 'empty' 7-state HMM, i.e. one where all params are set to zero\n",
    "\n",
    "init_probs_7_state_tbc_long = [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00]\n",
    "\n",
    "trans_probs_7_state_tbc_long = [\n",
    "    [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00],\n",
    "    [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00],\n",
    "    [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00],\n",
    "    [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00],\n",
    "    [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00],\n",
    "    [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00],\n",
    "    [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00],\n",
    "]\n",
    "\n",
    "emission_probs_7_state_tbc_long = [\n",
    "    #   A     C     G     T\n",
    "    [0.00, 0.00, 0.00, 0.00],\n",
    "    [0.00, 0.00, 0.00, 0.00],\n",
    "    [0.00, 0.00, 0.00, 0.00],\n",
    "    [0.00, 0.00, 0.00, 0.00],\n",
    "    [0.00, 0.00, 0.00, 0.00],\n",
    "    [0.00, 0.00, 0.00, 0.00],\n",
    "    [0.00, 0.00, 0.00, 0.00],\n",
    "]\n",
    "\n",
    "hmm_7_state_tbc_long = hmm(init_probs_7_state_tbc_long, trans_probs_7_state_tbc_long, emission_probs_7_state_tbc_long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now adjust the parameters of the above model to reflect the training data (`x_long`, `z_long`). You can still probably do this by hand, but since the sequences are longer, you might want to implement some code to assist you in counting.\n",
    "\n",
    "**Explain to another student how you do this, and see section 4 below for how to implement code for training-by-counting.**\n",
    "\n",
    "Adjust the parameters in the above declaration of `hmm_7_state_tbc_long` to the parameters that you get by training-by-counting. (Remember to validate that they are legal parameters.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code from doing training-by-counting using (x_long, z_long) as training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now Compute the probability of the Viterbi decoding of `x_short` using your trained model and the 'original' 7-state model from the previous weeks, i.e.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Probability of the Viterbi decoding of x_short using hmm_7_state_tbc_long\n",
    "w = compute_w_log(hmm_7_state_tbc_long, translate_observations_to_indices(x_short))\n",
    "print opt_path_prob_log(w)\n",
    "\n",
    "# Probability of the Viterbi decoding of x_short using hmm_7_state\n",
    "w = compute_w_log(hmm_7_state, translate_observations_to_indices(x_short))\n",
    "print opt_path_prob_log(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do the two probabilities compare? What do you expect?\n",
    "\n",
    "Now compute the probability of the Viterbi decoding of `x_long` using the same two models, i.e.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Probability of the Viterbi decoding of x_long using short_hmm_7_state_tbc_long\n",
    "w = compute_w_log(hmm_7_state_tbc_long, translate_observations_to_indices(x_long))\n",
    "print opt_path_prob_log(w)\n",
    "\n",
    "# Probability of the Viterbi decoding of x_long using hmm_7_state\n",
    "w = compute_w_log(hmm_7_state, translate_observations_to_indices(x_long))\n",
    "print opt_path_prob_log(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do the two probabilities compare? What do you expect?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 - Training-by-counting in general"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training a hidden Markov model is a matter of estimating the initial, transition and emission probabilities. If we are given training data, i.e. a sequence of observations, ${\\bf X}$, and a corresponding sequence of hidden states, ${\\bf Z}$, we can do \"training by counting\" by counting the number of observed the transitions and observations in the training dataand as explained in the lecture.\n",
    "\n",
    "Given ${\\bf X}$ and ${\\bf Z}$ we would like to count the number of transitions from one state to another, and the number of times that symbol $k$ was observed while being in state $i$.  That is, we want to construct a $K \\times K$ matrix such that entry $i, j$ is the number of times that a transition from state $i$ to state $j$ is observed in the training data, and a $K \\times D$ matrix where entry $i, k$ contains the number of times that symbol $k$ is observed in the training data while being in state $i$.\n",
    "\n",
    "Implement this as the below function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_transitions_and_emissions(K, D, x, z):\n",
    "    \"\"\"\n",
    "    Returns a KxK matrix and a KxD matrix containing counts cf. above\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your implementation of `count_transitions_and_emissions` on (prefixes) of `x_long` and `z_long` above in order to conclude that your implementation works as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code here ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use your implementation of `count_transitions_and_emissions` to implement a function `training_by_counting` that given the number of hidden states, $K$, the number of observables, $D$, a sequence of observations, ${\\bf X}$, and a corresponding sequence of hidden states, ${\\bf Z}$, returns a HMM (as an instance of `class hmm`), where the tranistion, emission, and initial probabilities are set cf. training by counting on ${\\bf X}$ and ${\\bf Z}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def training_by_counting(K, D, x, z):\n",
    "    \"\"\"\n",
    "    Returns a HMM trained on x and z cf. training-by-counting.\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now construct a HMM trained on `x_long` and `z_long` as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hmm_7_state_tbc_long = training_by_counting(7, 4, x_long, z_long)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
